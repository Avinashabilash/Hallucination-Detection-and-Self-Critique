{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0974b30c-01b1-4d4a-82a3-13c4b1f7e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Basic imports and device\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f03e1034-f9fe-45b8-8587-eea19bdadb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you need deps, run: pip install -r requirements.txt\n"
     ]
    }
   ],
   "source": [
    "print('If you need deps, run: pip install -r requirements.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c18480-5d2d-4123-ab7e-83e16af551d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/flan-t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and pipeline ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load a small text2text model (Flan-T5 small) as an example\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "MODEL = 'google/flan-t5-small'\n",
    "print('Loading', MODEL)\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL).to(DEVICE)\n",
    "    gen = pipeline('text2text-generation', model=model, tokenizer=tokenizer, device=0 if DEVICE=='cuda' else -1)\n",
    "    print('Model and pipeline ready')\n",
    "except Exception as e:\n",
    "    print('Could not load model automatically in this environment. You can still prepare prompts and run on a machine with internet/GPU.')\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d980126-19a3-4187-b2e8-55c5dcecb932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ERROR: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.\"]\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Simple generation helper\n",
    "def generate_answer(prompt, max_len=128, temperature=0.7, num_return_sequences=1):\n",
    "    try:\n",
    "        out = gen(prompt, max_length=max_len, do_sample=True, temperature=temperature, num_return_sequences=num_return_sequences)\n",
    "        return [o['generated_text'].strip() for o in out]\n",
    "    except Exception as e:\n",
    "        return [f'ERROR: {e}']\n",
    "\n",
    "# quick test (works only if model loaded)\n",
    "print(generate_answer('Q: Who wrote the novel \"1984\"?\\nA:', max_len=64, temperature=0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b924e8-dbaa-413b-8f20-6c843e83ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abiav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "FAISS demo ready — try retrieve_top_k('Who wrote 1984?')\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: FAISS demo with sentence-transformers (tiny corpus)\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "    EMB_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embedder = SentenceTransformer(EMB_MODEL)\n",
    "    corpus = [\n",
    "        'George Orwell was an English novelist, essayist, and critic best known for 1984 and Animal Farm.',\n",
    "        'The capital of France is Paris.',\n",
    "        'Python is a programming language created by Guido van Rossum.'\n",
    "    ]\n",
    "    corpus_emb = embedder.encode(corpus, convert_to_numpy=True)\n",
    "    index = faiss.IndexFlatL2(corpus_emb.shape[1])\n",
    "    index.add(corpus_emb)\n",
    "    def retrieve_top_k(query, k=3):\n",
    "        q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "        D, I = index.search(q_emb, k)\n",
    "        return [corpus[i] for i in I[0]]\n",
    "    print('FAISS demo ready — try retrieve_top_k(\\'Who wrote 1984?\\')')\n",
    "except Exception as e:\n",
    "    print('FAISS or sentence-transformers not available in this kernel. Install requirements or run on a machine with internet.')\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56b145b-d0f2-47ee-aa21-8fe1ca52e36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"ERROR: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.\", ['George Orwell was an English novelist, essayist, and critic best known for 1984 and Animal Farm.', 'Python is a programming language created by Guido van Rossum.', 'The capital of France is Paris.'])\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Simple RAG: retrieve + prompt generator\n",
    "def rag_generate(question, k=3):\n",
    "    try:\n",
    "        contexts = retrieve_top_k(question, k=k)\n",
    "    except Exception:\n",
    "        contexts = []\n",
    "    combined_context = '\\\\n'.join([f\"Evidence: {c}\" for c in contexts])\n",
    "    prompt = f\"{combined_context}\\\\nQ: {question}\\\\nA: Provide a concise, evidence-backed answer.\"\n",
    "    ans = generate_answer(prompt, max_len=128, temperature=0.0)[0]\n",
    "    return ans, contexts\n",
    "\n",
    "# Example (works if retrieval + generation exist)\n",
    "print(rag_generate('Who wrote the novel 1984?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225b9a55-b3b5-40da-a4f5-323140d45582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Self-critique helper (ask the model to critique its answer)\n",
    "def self_critique(question, answer):\n",
    "    critique_prompt = (\n",
    "        f\"Q: {question}\\\\nA: {answer}\\\\n\\\\nPlease list any claims in the answer that might be uncertain or require citation. For each claim, say whether you are confident and if not, ask for evidence.\"\n",
    "    )\n",
    "    critique = generate_answer(critique_prompt, max_len=200, temperature=0.0)[0]\n",
    "    return critique\n",
    "\n",
    "# Example\n",
    "print(self_critique('Who wrote 1984?', 'The novel 1984 was written by George Orwell.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34df994d-f059-40f8-baac-d5b82d59b2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results.csv to C:\\Users\\abiav\\hallucination_cuda\\results.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save results & export stub\n",
    "import pandas as pd\n",
    "results = [{'id':1, 'question':'Who wrote 1984?', 'baseline':'George Orwell', 'rag':'George Orwell'}]\n",
    "df = pd.DataFrame(results)\n",
    "fn = Path('results.csv')\n",
    "df.to_csv(fn, index=False)\n",
    "print('Saved results.csv to', fn.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b4610-ed94-4021-bab4-5f1d64ea24d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
